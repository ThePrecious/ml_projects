{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multivariate Timeseries Data Modeling to predict room occupancy with Gluon\n",
    "\n",
    "@author: Amulya Badal\n",
    "\n",
    "In this notebook I attempt my first neural network with Gluon to model timeseries data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET\n",
    "Occupancy Detection Dataset\n",
    "This dataset describes measurements of a room and the objective is to predict whether or not the room is occupied.\n",
    "\n",
    "There are 20,560 one-minute observations taken over the period of a few weeks. This is a classification prediction problem. There are 7 attributes including various light and climate properties of the room.\n",
    "\n",
    "The source for the data is credited to Luis Candanedo from UMONS.\n",
    "\n",
    "Below is a sample of the first 5 rows of data including the header row.\n",
    "\n",
    "```\n",
    "\n",
    "\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"\n",
    "\"1\",\"2015-02-04 17:51:00\",23.18,27.272,426,721.25,0.00479298817650529,1\n",
    "\"2\",\"2015-02-04 17:51:59\",23.15,27.2675,429.5,714,0.00478344094931065,1\n",
    "\"3\",\"2015-02-04 17:53:00\",23.15,27.245,426,713.5,0.00477946352442199,1\n",
    "\"4\",\"2015-02-04 17:54:00\",23.15,27.2,426,708.25,0.00477150882608175,1\n",
    "\"5\",\"2015-02-04 17:55:00\",23.1,27.2,426,704.5,0.00475699293331518,1\n",
    "\"6\",\"2015-02-04 17:55:59\",23.1,27.2,419,701,0.00475699293331518,1\n",
    "\n",
    "```\n",
    "\n",
    "The data is provided in 3 files that suggest the splits that may be used for training and testing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amulya/anaconda2/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "#1. Identify what's the feature in the examples  \n",
    "X = [\n",
    "    \"date\",\n",
    "    \"Temperature\",\n",
    "    \"Humidity\",\n",
    "    \"Light\",\n",
    "    \"CO2\",\n",
    "    \"HumidityRatio\"\n",
    "]\n",
    "\n",
    "#2. Target variable is occupancy\n",
    "y = [\n",
    "    \"Occupancy\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** TRAIN DATA *****\n",
      "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
      "1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
      "2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
      "3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
      "4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
      "5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
      "\n",
      "   Occupancy  \n",
      "1          1  \n",
      "2          1  \n",
      "3          1  \n",
      "4          1  \n",
      "5          1  \n"
     ]
    }
   ],
   "source": [
    "# Use pandas to load the data\n",
    "import pandas as pd\n",
    "\n",
    "training_file_path = \"occupancy_data/datatraining.txt\"\n",
    "test_file_path = \"occupancy_data/datatest.txt\" \n",
    "test2_file_path = \"occupancy_data/datatest2.txt\" \n",
    "\n",
    "# TRAINING DATA\n",
    "print(\"***** TRAIN DATA *****\")\n",
    "\n",
    "col_names = [\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"]\n",
    "data_features = [\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\"]\n",
    "\n",
    "train_data = pd.read_csv(training_file_path, skiprows=[0], names=col_names)\n",
    "print train_data.head()\n",
    "\n",
    "#VALIDATION DATA\n",
    "val_data = pd.read_csv(test_file_path, sep=\",\", skiprows=[0], header=None, names=col_names)\n",
    "\n",
    "#TEST DATA\n",
    "test_data = pd.read_csv(test2_file_path, sep=\",\", skiprows=[0], header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Training DATA *****\n",
      "(8143, 6)\n",
      "(8143,)\n",
      "***** Validation DATA *****\n",
      "(2665, 6)\n",
      "(2665,)\n",
      "***** TEST DATA *****\n",
      "(9752, 6)\n",
      "(9752,)\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Training DATA *****\")\n",
    "X_train_raw = train_data[data_features]\n",
    "print(X_train_raw.shape)\n",
    "\n",
    "y_train = train_data.Occupancy\n",
    "print(y_train.shape)\n",
    "\n",
    "# VAL\n",
    "print(\"***** Validation DATA *****\")\n",
    "\n",
    "X_val_raw = val_data[data_features]\n",
    "print(X_val_raw.shape)\n",
    "\n",
    "y_val = val_data.Occupancy\n",
    "print(y_val.shape)\n",
    "\n",
    "print(\"***** TEST DATA *****\")\n",
    "# Test\n",
    "\n",
    "X_test_raw = test_data[data_features]\n",
    "print(X_test_raw.shape)\n",
    "\n",
    "y_test = test_data.Occupancy\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop NaN values - always a good idea to remove NaNs from the dataset\n",
    "#train_data.dropna(axis=0, how='any')\n",
    "#val_data.dropna(axis=0, how='any')\n",
    "#test2_data.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'date', u'Temperature', u'Humidity', u'Light', u'CO2',\n",
      "       u'HumidityRatio', u'Occupancy'],\n",
      "      dtype='object')\n",
      "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
      "1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
      "2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
      "3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
      "4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
      "5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
      "\n",
      "   Occupancy  \n",
      "1          1  \n",
      "2          1  \n",
      "3          1  \n",
      "4          1  \n",
      "5          1  \n"
     ]
    }
   ],
   "source": [
    "print (train_data.columns)\n",
    "print (train_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Lets add new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to one-hot encoding\n",
    "- The house is likely to be occupied more on the weekends than weekdays, lets use that as an explicit new feature\n",
    "- People are likely to be in their house in the evening and night, lets create time buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Function to create new features\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_new_features(data):\n",
    "    time_buckets = []\n",
    "    weekday = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        parsed_date = datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        time_bucket = parsed_date.hour % 4 # lets divide in to [day 6-12, afternoon 12-6, evening 6-12, night 12-6]\n",
    "        time_buckets.append(time_bucket)\n",
    "        wday = 0 if parsed_date.isoweekday() > 5 else 1\n",
    "        weekday.append(wday)\n",
    "    return time_buckets, weekday\n",
    "    \n",
    "train_time_buckets, train_weekday = create_new_features(train_data)\n",
    "val_time_buckets, val_weekday = create_new_features(val_data)\n",
    "test_time_buckets, test_weekday = create_new_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation dataset did not have any values with occupancy 0. So we are adding one here\n",
    "val_weekday[-1]=0 #dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### One hot encoding of the Weekday and Time buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.]]), array([0, 1, 2, 3]))\n",
      "(array([[ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.]]), array([0, 1]))\n",
      "(array([[ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.]]), array([0, 1, 2, 3]))\n",
      "(array([[ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.]]), array([0, 1]))\n",
      "(array([[ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.]]), array([0, 1, 2, 3]))\n",
      "(array([[ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.]]), array([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_encode(values):\n",
    "    new_values = np.array(values)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(new_values)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return (onehot_encoded, label_encoder.classes_)\n",
    "\n",
    "## TRAINING DATA\n",
    "train_time_bucket_onehot_encoded, train_time_class = onehot_encode(train_time_buckets)\n",
    "train_weekday_onehot_encoded, train_weekday_class = onehot_encode(train_weekday)\n",
    "print (train_time_bucket_onehot_encoded, train_time_class)\n",
    "print (train_weekday_onehot_encoded, train_weekday_class)\n",
    "\n",
    "\n",
    "## VALIDATION DATA\n",
    "val_time_bucket_onehot_encoded, val_time_class = onehot_encode(val_time_buckets)\n",
    "val_weekday_onehot_encoded, val_weekday_class = onehot_encode(val_weekday)\n",
    "print (val_time_bucket_onehot_encoded, val_time_class)\n",
    "print (val_weekday_onehot_encoded, val_weekday_class)\n",
    "\n",
    "## TEST DATA\n",
    "\n",
    "test_time_bucket_onehot_encoded, test_time_class = onehot_encode(test_time_buckets)\n",
    "test_weekday_onehot_encoded, test_weekday_class = onehot_encode(test_weekday)\n",
    "print (test_time_bucket_onehot_encoded, test_time_class)\n",
    "print (test_weekday_onehot_encoded, test_weekday_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8143, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append new features to Train, Validation and Test sets\n",
    "\n",
    "def add_new_features(arr, w_oh, t_oh):\n",
    "    # lets ignore the date column for all the rows\n",
    "    X = arr.values[:, 1:] \n",
    "    XW = np.hstack((X, w_oh))\n",
    "    XTW = np.hstack((XW, t_oh))\n",
    "    return XTW\n",
    "\n",
    "X_train = add_new_features(X_train_raw, train_weekday_onehot_encoded, train_time_bucket_onehot_encoded)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare validation data\n",
    "\n",
    "X_val = add_new_features(X_val_raw, val_weekday_onehot_encoded, val_time_bucket_onehot_encoded)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9752, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = add_new_features(X_test_raw, test_weekday_onehot_encoded, test_time_bucket_onehot_encoded)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9752,), 2049, 0.78988925348646433)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of positive occupancy\n",
    "y_test.shape, sum(y_test), 1-sum(y_test)/(y_test.shape[0]*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BASERNN Class \n",
    "# This code is copied from https://github.com/sunilmallya/timeseries/blob/master/sagemaker-timeseries/generic_ts.py\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import math\n",
    "from mxnet import nd, autograd\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "class BaseRNNClassifier(mx.gluon.Block):\n",
    "    '''\n",
    "    Extensible RNN class with LSTM that can operate with MXNet NDArray iter or DataLoader.\n",
    "    Includes fit() function to mimic the symbolic fit() function\n",
    "    '''\n",
    "    \n",
    "    @classmethod\n",
    "    def get_data(cls, batch, iter_type, ctx):\n",
    "        ''' get data and label from the iterator/dataloader '''\n",
    "        if iter_type == 'mxiter':\n",
    "            X = batch.data[0].as_in_context(ctx)\n",
    "            y = batch.label[0].as_in_context(ctx)\n",
    "        elif iter_type in [\"numpy\", \"dataloader\"]:\n",
    "            X = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "        else:\n",
    "            raise ValueError(\"iter_type must be mxiter or numpy\")\n",
    "        return X, y\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_labels(cls, data_iterator, iter_type):\n",
    "        if iter_type == 'mxiter':\n",
    "            pass\n",
    "        elif iter_type in [\"numpy\", \"dataloader\"]:\n",
    "            return data_iterator._dataset._label\n",
    "    \n",
    "    def __init__(self, ctx):\n",
    "        super(BaseRNNClassifier, self).__init__()\n",
    "        self.ctx = ctx\n",
    "\n",
    "    def build_model(self, n_out, rnn_size=128, n_layer=1):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        # LSTM default; #TODO(Sunil): make this generic\n",
    "        self.lstm = mx.gluon.rnn.LSTM(self.rnn_size, self.n_layer, layout='NTC')\n",
    "        #self.lstm = mx.gluon.rnn.GRU(self.rnn_size, self.n_layer)\n",
    "        self.output = mx.gluon.nn.Dense(self.n_out)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out[:, out.shape[1]-1, :]\n",
    "        out = self.output(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def compile_model(self, loss=None, lr=3E-3):\n",
    "        self.collect_params().initialize(mx.init.Xavier(), ctx=self.ctx)\n",
    "        self.criterion = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "        self.loss = mx.gluon.loss.SoftmaxCrossEntropyLoss() if loss is None else loss\n",
    "        self.lr = lr\n",
    "        self.optimizer = mx.gluon.Trainer(self.collect_params(), 'adam', \n",
    "                                          {'learning_rate': self.lr})\n",
    "\n",
    "    def top_k_acc(self, data_iterator, iter_type='mxiter', top_k=3, batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        true_labels = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            batch_pred = self.forward(data, hidden)\n",
    "            #batch_pred = mx.nd.argmax(batch_pred, axis=1)\n",
    "            batch_pred_list.append(batch_pred.asnumpy())\n",
    "            true_labels.append(label)\n",
    "        y = np.vstack(batch_pred_list)\n",
    "        true_labels = np.vstack(true_labels)\n",
    "        argsorted_y = np.argsort(y)[:,-top_k:]\n",
    "        return np.asarray(np.any(argsorted_y.T == true_labels, axis=0).mean(dtype='f'))\n",
    "    \n",
    "    def evaluate_accuracy(self, data_iterator, metric='acc', iter_type='mxiter', batch_size=128):\n",
    "        met = mx.metric.Accuracy()\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            # Lets do a forward pass only!\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            preds = mx.nd.argmax(output, axis=1)\n",
    "            met.update(labels=label, preds=preds)\n",
    "                \n",
    "        #if self.all_labels is None:\n",
    "        #    self.all_labels = BaseRNNClassifier.get_all_labels(data_iterator, iter_type)\n",
    "        #preds = self.predict(data_iterator, iter_type=iter_type, batch_size=batch_size)\n",
    "        #met.update(labels=mx.nd.array(self.all_labels[:len(preds)]), preds=preds)\n",
    "        \n",
    "        return met.get()                   \n",
    "                    \n",
    "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            batch_pred_list.append(output.asnumpy())\n",
    "        #return np.vstack(batch_pred_list)\n",
    "        return np.argmax(np.vstack(batch_pred_list), 1)\n",
    "    \n",
    "    def fit(self, train_data, test_data, epochs, batch_size, verbose=True):\n",
    "        '''\n",
    "        @train_data:  can be of type list of Numpy array, DataLoader, MXNet NDArray Iter\n",
    "        '''\n",
    "        \n",
    "        moving_loss = 0.\n",
    "        total_batches = 0\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "\n",
    "        iter_type = 'numpy'\n",
    "        train_iter = None\n",
    "        test_iter = None\n",
    "        print \"Data type:\", type(train_data), type(test_data), iter_type, type(train_data[0])\n",
    "        \n",
    "        # Can take MX NDArrayIter, or DataLoader\n",
    "        if isinstance(train_data, mx.io.NDArrayIter):\n",
    "            train_iter = train_data\n",
    "            test_iter = test_data\n",
    "            iter_type = 'mxiter'\n",
    "            #total_batches = train_iter.num_data // train_iter.batch_size\n",
    "\n",
    "        elif isinstance(train_data, list):\n",
    "            if True: #dummy\n",
    "            #if isinstance(train_data[0], np.ndarray) and isinstance(train_data[1], np.ndarray):\n",
    "                X, y = np.asarray(train_data[0]).astype('float32'), np.asarray(train_data[1]).astype('float32')\n",
    "                tX, ty = np.asarray(test_data[0]).astype('float32'), np.asarray(test_data[1]).astype('float32')\n",
    "                \n",
    "                total_batches = X.shape[0] // batch_size\n",
    "                train_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y), \n",
    "                                    batch_size=batch_size, shuffle=True, last_batch='discard')\n",
    "                test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(tX, ty), \n",
    "                                    batch_size=batch_size, shuffle=False, last_batch='discard')\n",
    "                \n",
    "        elif isinstance(train_data, mx.gluon.data.dataloader.DataLoader) and isinstance(test_data, mx.gluon.data.dataloader.DataLoader):\n",
    "            train_iter = train_data\n",
    "            test_iter = test_data\n",
    "            total_batches = len(train_iter)\n",
    "        else:\n",
    "            raise ValueError(\"pass mxnet ndarray or numpy array as [data, label]\")\n",
    "\n",
    "        print \"Data type:\", type(train_data), type(test_data), iter_type\n",
    "        print \"Sizes\", self.n_layer, batch_size, self.rnn_size, self.ctx\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            #print self.lstm.collect_params()\n",
    "\n",
    "            # reset iterators if of MXNet Itertype\n",
    "            if iter_type == \"mxiter\":\n",
    "                train_iter.reset()\n",
    "                test_iter.reset()\n",
    "        \n",
    "            init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "            hidden = [init_state] * 2                \n",
    "            #hidden = self.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=self.ctx)\n",
    "            yhat = []\n",
    "            for i, batch in enumerate(train_iter):\n",
    "                data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "                #print \"Data Shapes:\", data.shape, label.shape\n",
    "                hidden = detach(hidden)\n",
    "                with mx.autograd.record(train_mode=True):\n",
    "                    preds, hidden = self.forward(data, hidden)\n",
    "                    #print preds[0].shape, hidden[0].shape, label.shape\n",
    "                    loss = self.loss(preds, label) \n",
    "                    yhat.extend(preds)\n",
    "                loss.backward()                                        \n",
    "                self.optimizer.step(batch_size)\n",
    "                preds = mx.nd.argmax(preds, axis=1)\n",
    "                \n",
    "                batch_acc = mx.nd.mean(preds == label).asscalar()\n",
    "\n",
    "                if i == 0:\n",
    "                    moving_loss = nd.mean(loss).asscalar()\n",
    "                else:\n",
    "                    moving_loss = .99 * moving_loss + .01 * mx.nd.mean(loss).asscalar()\n",
    "                    \n",
    "                if verbose and i%100 == 0:\n",
    "                    print('[Epoch {}] [Batch {}/{}] Loss: {:.5f}, Batch acc: {:.5f}'.format(\n",
    "                          e, i, total_batches, moving_loss, batch_acc))                    \n",
    "                    \n",
    "            train_loss.append(moving_loss)\n",
    "            \n",
    "            t_acc = self.evaluate_accuracy(train_iter, iter_type=iter_type, batch_size=batch_size)\n",
    "            train_acc.append(t_acc[1])\n",
    "            \n",
    "            tst_acc = self.evaluate_accuracy(test_iter, iter_type=iter_type, batch_size=batch_size)\n",
    "            test_acc.append(tst_acc[1])\n",
    "\n",
    "            print(\"Epoch %s. Loss: %.5f Train Acc: %s Test Acc: %s\" % (e, moving_loss, t_acc, tst_acc))\n",
    "        return train_loss, train_acc, test_acc\n",
    "                    \n",
    "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            batch_pred_list.append(output.asnumpy())\n",
    "        return np.argmax(np.vstack(batch_pred_list), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8143, 11), (8143,), (2665, 11), (2665,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8143, 1, 11)\n",
      "(2665, 1, 11)\n",
      "(9752, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "# Expand dimensions\n",
    "\n",
    "# RNNs take inputs as (batch_size, time step or step_size or sequence_length, features)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "print X_train.shape\n",
    "\n",
    "X_val = np.expand_dims(X_val, axis=1)\n",
    "print X_val.shape\n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <type 'list'> <type 'list'> numpy <type 'numpy.ndarray'>\n",
      "Data type: <type 'list'> <type 'list'> numpy\n",
      "Sizes 1 32 8 cpu(0)\n",
      "[Epoch 0] [Batch 0/254] Loss: 0.63319, Batch acc: 0.78125\n",
      "[Epoch 0] [Batch 100/254] Loss: 0.51897, Batch acc: 0.71875\n",
      "[Epoch 0] [Batch 200/254] Loss: 0.43227, Batch acc: 0.84375\n",
      "Epoch 0. Loss: 0.39591 Train Acc: ('accuracy', 0.7947834645669292) Test Acc: ('accuracy', 0.8123972039473685)\n",
      "[Epoch 1] [Batch 0/254] Loss: 0.35631, Batch acc: 0.90625\n",
      "[Epoch 1] [Batch 100/254] Loss: 0.33867, Batch acc: 0.81250\n",
      "[Epoch 1] [Batch 200/254] Loss: 0.30496, Batch acc: 1.00000\n",
      "Epoch 1. Loss: 0.28885 Train Acc: ('accuracy', 0.9400836614173228) Test Acc: ('accuracy', 0.9272203947368421)\n",
      "[Epoch 2] [Batch 0/254] Loss: 0.15314, Batch acc: 1.00000\n",
      "[Epoch 2] [Batch 100/254] Loss: 0.21336, Batch acc: 0.87500\n",
      "[Epoch 2] [Batch 200/254] Loss: 0.22482, Batch acc: 0.93750\n",
      "Epoch 2. Loss: 0.22412 Train Acc: ('accuracy', 0.9542322834645669) Test Acc: ('accuracy', 0.9435649671052632)\n",
      "[Epoch 3] [Batch 0/254] Loss: 0.30454, Batch acc: 0.90625\n",
      "[Epoch 3] [Batch 100/254] Loss: 0.24046, Batch acc: 0.93750\n",
      "[Epoch 3] [Batch 200/254] Loss: 0.20337, Batch acc: 0.90625\n",
      "Epoch 3. Loss: 0.18263 Train Acc: ('accuracy', 0.984867125984252) Test Acc: ('accuracy', 0.9712171052631579)\n",
      "[Epoch 4] [Batch 0/254] Loss: 0.18502, Batch acc: 0.96875\n",
      "[Epoch 4] [Batch 100/254] Loss: 0.15585, Batch acc: 0.96875\n",
      "[Epoch 4] [Batch 200/254] Loss: 0.13556, Batch acc: 1.00000\n",
      "Epoch 4. Loss: 0.12641 Train Acc: ('accuracy', 0.9886811023622047) Test Acc: ('accuracy', 0.9809827302631579)\n",
      "[Epoch 5] [Batch 0/254] Loss: 0.08496, Batch acc: 1.00000\n",
      "[Epoch 5] [Batch 100/254] Loss: 0.09987, Batch acc: 1.00000\n",
      "[Epoch 5] [Batch 200/254] Loss: 0.09969, Batch acc: 1.00000\n",
      "Epoch 5. Loss: 0.09696 Train Acc: ('accuracy', 0.9889271653543307) Test Acc: ('accuracy', 0.9808799342105263)\n",
      "[Epoch 6] [Batch 0/254] Loss: 0.16421, Batch acc: 0.96875\n",
      "[Epoch 6] [Batch 100/254] Loss: 0.11811, Batch acc: 0.96875\n",
      "[Epoch 6] [Batch 200/254] Loss: 0.09747, Batch acc: 1.00000\n",
      "Epoch 6. Loss: 0.09138 Train Acc: ('accuracy', 0.9888041338582677) Test Acc: ('accuracy', 0.9819078947368421)\n",
      "[Epoch 7] [Batch 0/254] Loss: 0.05011, Batch acc: 1.00000\n",
      "[Epoch 7] [Batch 100/254] Loss: 0.07186, Batch acc: 1.00000\n",
      "[Epoch 7] [Batch 200/254] Loss: 0.07203, Batch acc: 1.00000\n",
      "Epoch 7. Loss: 0.07041 Train Acc: ('accuracy', 0.9889271653543307) Test Acc: ('accuracy', 0.9815995065789473)\n",
      "[Epoch 8] [Batch 0/254] Loss: 0.03882, Batch acc: 1.00000\n",
      "[Epoch 8] [Batch 100/254] Loss: 0.05974, Batch acc: 0.96875\n",
      "[Epoch 8] [Batch 200/254] Loss: 0.06646, Batch acc: 0.93750\n",
      "Epoch 8. Loss: 0.06484 Train Acc: ('accuracy', 0.9890501968503937) Test Acc: ('accuracy', 0.9802631578947368)\n",
      "[Epoch 9] [Batch 0/254] Loss: 0.05653, Batch acc: 0.96875\n",
      "[Epoch 9] [Batch 100/254] Loss: 0.06225, Batch acc: 1.00000\n",
      "[Epoch 9] [Batch 200/254] Loss: 0.06385, Batch acc: 1.00000\n",
      "Epoch 9. Loss: 0.06513 Train Acc: ('accuracy', 0.9889271653543307) Test Acc: ('accuracy', 0.9717310855263158)\n"
     ]
    }
   ],
   "source": [
    "N_CLASS = 2\n",
    "\n",
    "ctx = mx.cpu(0) #change context to execute on CPU\n",
    "model = BaseRNNClassifier(ctx)\n",
    "model.build_model(n_out=N_CLASS, rnn_size=8, n_layer=1)\n",
    "model.compile_model()\n",
    "#train_loss, train_acc, test_acc = model.fit([X_train[1:], y_train[1:]], [X_test, y_test], batch_size=32, epochs=25)\n",
    "\n",
    "#train_loss, train_acc, test_acc = model.fit([X_train, y_train], [X_val, y_val], batch_size=32, epochs=4)\n",
    "train_loss, train_acc, test_acc = model.fit([X_train, y_train], [X_test, y_test], batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 24\n",
    "tX, ty = np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32')\n",
    "test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(tX, ty), \n",
    "                                    batch_size=b_size, shuffle=False, last_batch='discard')\n",
    "pred_out = model.predict(test_iter, iter_type=\"dataloader\", batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7558  145]\n",
      " [ 131 1910]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print confusion_matrix(y_test[:len(pred_out)], pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d106ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Accuracy with every epoch\")\n",
    "plt.plot(train_acc, label=\"train\")\n",
    "plt.plot(test_acc, label =\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4ohCX0Qkjij0RUTiiEJfRCSO/H9wX7n22LWsFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7fe350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss function\")\n",
    "plt.plot(train_loss, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (9752, 11)\n",
      "Train Acc: 0.811761811024 Test Acc: 0.860711348684\n",
      "Train Acc: 0.805241141732 Test Acc: 0.858963815789\n",
      "Train Acc: 0.919045275591 Test Acc: 0.928556743421\n",
      "Train Acc: 0.926796259843 Test Acc: 0.937602796053\n",
      "Train Acc: 0.918553149606 Test Acc: 0.927425986842\n",
      "Train Acc: 0.918184055118 Test Acc: 0.926295230263\n",
      "Train Acc: 0.909079724409 Test Acc: 0.906352796053\n",
      "Train Acc: 0.919168307087 Test Acc: 0.928556743421\n",
      "Train Acc: 0.923720472441 Test Acc: 0.93359375\n",
      "Train Acc: 0.918184055118 Test Acc: 0.925164473684\n",
      "Train Acc: 0.912893700787 Test Acc: 0.912828947368\n",
      "Train Acc: 0.918184055118 Test Acc: 0.925472861842\n",
      "Train Acc: 0.908956692913 Test Acc: 0.905736019737\n",
      "Train Acc: 0.915600393701 Test Acc: 0.919407894737\n",
      "Train Acc: 0.905757874016 Test Acc: 0.902240953947\n",
      "Train Acc: 0.920644685039 Test Acc: 0.948087993421\n",
      "Train Acc: 0.849163385827 Test Acc: 0.895970394737\n",
      "Train Acc: 0.916584645669 Test Acc: 0.949527138158\n",
      "Train Acc: 0.918184055118 Test Acc: 0.924856085526\n",
      "Train Acc: 0.89751476378 Test Acc: 0.944695723684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print X_test.shape\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import math\n",
    "from mxnet import nd, autograd\n",
    "\n",
    "N_CLASS = 2\n",
    "ctx = mx.cpu()\n",
    "net = mx.gluon.nn.Dense(N_CLASS)\n",
    "\n",
    "#INIT\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=ctx)\n",
    "softmax_cross_entropy_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "\n",
    "## accuracy function\n",
    "def evaluate_accuracy(t_data, t_label, net, ctx=mx.cpu()):\n",
    "    num_correct = 0.0\n",
    "    num_total = len(t_data)\n",
    "    batch_size = 32\n",
    "    n_batches = num_total // batch_size\n",
    "    acc = mx.metric.Accuracy()\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        data = mx.nd.array(t_data[i*batch_size:(i+1)*batch_size])\n",
    "        label = mx.nd.array(t_label[i*batch_size:(i+1)*batch_size])\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "###\n",
    "    \n",
    "\n",
    "epochs = 20\n",
    "moving_loss = 0.\n",
    "\n",
    "batch_size = 32\n",
    "n_batches = len(X_train)//batch_size\n",
    "loss_sequence = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        data = mx.nd.array(X_train[i*batch_size:(i+1)*batch_size])\n",
    "        label = mx.nd.array(y_train[i*batch_size:(i+1)*batch_size])\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        # Step1. Lets do the forward pass\n",
    "        with mx.autograd.record():\n",
    "            output = net(data)\n",
    "            \n",
    "            # Step2. Compute the Loss\n",
    "            loss = softmax_cross_entropy_loss(output, label)\n",
    "            \n",
    "        # Step3. Do the backward pass, update the weights    \n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_sequence.append(cumulative_loss)\n",
    "    #print e, cumulative_loss\n",
    "    \n",
    "    # Step4. Compute the test accuracy\n",
    "    test_accuracy = evaluate_accuracy(X_test, y_test, net)\n",
    "    train_accuracy = evaluate_accuracy(X_train, y_train, net)\n",
    "        \n",
    "    print \"Train Acc:\", train_accuracy, \"Test Acc:\", test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best accuracy seems to be 94.4%, the RNN achieves a much better accuracy at 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
